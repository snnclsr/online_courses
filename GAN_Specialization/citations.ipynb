{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1_W1\n",
    "\n",
    "All of the resources cited in Course 1 Week 1, in one place. You are encouraged to explore these papers/sites if they interest you! They are listed in the order they appear in the lessons.\n",
    "\n",
    "From the videos:\n",
    "\n",
    "* Hyperspherical Variational Auto-Encoders (Davidson, Falorsi, De Cao, Kipf, and Tomczak, 2018): https://www.researchgate.net/figure/Latent-space-visualization-of-the-10-MNIST-digits-in-2-dimensions-of-both-N-VAE-left_fig2_324182043\n",
    "* Analyzing and Improving the Image Quality of StyleGAN (Karras et al., 2020): https://arxiv.org/abs/1912.04958\n",
    "* Semantic Image Synthesis with Spatially-Adaptive Normalization (Park, Liu, Wang, and Zhu, 2019): https://arxiv.org/abs/1903.07291\n",
    "* Few-shot Adversarial Learning of Realistic Neural Talking Head Models (Zakharov, Shysheya, Burkov, and Lempitsky, 2019): https://arxiv.org/abs/1905.08233\n",
    "* Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling (Wu, Zhang, Xue, Freeman, and Tenenbaum, 2017): https://arxiv.org/abs/1610.07584\n",
    "* These Cats Do Not Exist (Glover and Mott, 2019): http://thesecatsdonotexist.com/\n",
    "\n",
    "From the notebooks:\n",
    "\n",
    "* Large Scale GAN Training for High Fidelity Natural Image Synthesis (Brock, Donahue, and Simonyan, 2019): https://arxiv.org/abs/1809.11096\n",
    "* PyTorch Documentation: https://pytorch.org/docs/stable/index.html#pytorch-documentation\n",
    "* MNIST Database: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1_W2\n",
    "\n",
    "All of the resources cited in Course 1 Week 2, in one place. You are encouraged to explore these papers/sites if they interest you—for this week, both papers have been included as optional readings! They are listed in the order they appear in the lessons.\n",
    "\n",
    "From the videos:\n",
    "\n",
    "* Deconvolution and Checkerboard Artifacts (Odena et al., 2016): http://doi.org/10.23915/distill.00003\n",
    "\n",
    "From the notebooks:\n",
    "\n",
    "* Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford, Metz, and Chintala, 2016): https://arxiv.org/abs/1511.06434\n",
    "* MNIST Database: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "* https://distill.pub/2016/deconv-checkerboard/\n",
    "* https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1_W3\n",
    "\n",
    "Interested in the papers behind the Wasserstein GAN with Gradient Penalty (WGAN-GP) you just implemented? Check them out! The first paper is the original WGAN paper and the second proposes GP (as well as weight clipping) to WGAN in order to enforce 1-Lipschitz continuity and improve stability.\n",
    "\n",
    "* Wasserstein GAN (Arjovsky, Chintala, and Bottou, 2017): https://arxiv.org/abs/1701.07875\n",
    "\n",
    "* Improved Training of Wasserstein GANs (Gulrajani et al., 2017): https://arxiv.org/abs/1704.00028\n",
    "\n",
    "Want another explanation of WGAN? This article provides a great walkthrough of how WGAN addresses the difficulties of training a traditional GAN with a focus on the loss functions.\n",
    "\n",
    "* From GAN to WGAN (Weng, 2017): https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html\n",
    "\n",
    "* In this notebook, you'll learn about and implement spectral normalization, a weight normalization technique to stabilize the training of the discriminator, as proposed in Spectral Normalization for Generative Adversarial Networks (Miyato et al. 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1_W4\n",
    "\n",
    "Fascinated by conditional GANs? Take a look at the paper that proposed it and the training/testing methods that were behind it!\n",
    "\n",
    "* Conditional Generative Adversarial Nets (Mirza and Osindero, 2014): https://arxiv.org/abs/1411.1784\n",
    "\n",
    "Want to learn more about controlling GAN generations using latent space? Check out this paper!\n",
    "\n",
    "* Interpreting the Latent Space of GANs for Semantic Face Editing (Shen, Gu, Tang, and Zhou, 2020): https://arxiv.org/abs/1907.10786\n",
    "\n",
    "All of the resources cited in Course 1 Week 4, in one place. You are encouraged to explore these papers/sites if they interest you—the first paper has already been included as an optional reading! They are listed in the order they appear in the lessons.\n",
    "\n",
    "\n",
    "From the notebooks:\n",
    "\n",
    "* MNIST Database: http://yann.lecun.com/exdb/mnist/\n",
    "* CelebFaces Attributes Dataset (CelebA): http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2_W1\n",
    "\n",
    "Want to know more about why Fréchet Inception Distance has overtaken Inception Score? This paper illustrates the problems with using Inception Score.\n",
    "\n",
    "* A Note on the Inception Score (Barratt and Sharma, 2018): https://arxiv.org/abs/1801.01973\n",
    "\n",
    "Intrigued about human evaluation and HYPE (Human eYe Perceptual Evaluation) of GANs? Learn more about this human benchmark in the paper! You may notice a familiar name among the authors ;)\n",
    "\n",
    "* HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models (Zhou et al., 2019): \n",
    "https://arxiv.org/abs/1904.01121\n",
    "\n",
    "Using precision and recall metrics on GANs pique your interest? See some specific methods in this paper! Note that you will learn all about StyleGAN in Week 3 of this course.\n",
    "\n",
    "* Improved Precision and Recall Metric for Assessing Generative Models (Kynkäänniemi, Karras, Laine, Lehtinen, and Aila, 2019): https://arxiv.org/abs/1904.06991\n",
    "\n",
    "Need a summary of FID and IS? Here are two great articles that recap both metrics!\n",
    "\n",
    "* Fréchet Inception Distance (Jean, 2018): https://nealjean.com/ml/frechet-inception-distance/\n",
    "\n",
    "* GAN — How to measure GAN performance? (Hui, 2018): https://medium.com/@jonathan_hui/gan-how-to-measure-gan-performance-64b988c47732\n",
    "\n",
    "\n",
    "All of the resources cited in Course 2 Week 1, in one place. You are encouraged to explore these papers/sites if they interest you! They are listed in the order they appear in the lessons.\n",
    "\n",
    "From the videos:\n",
    "\n",
    "* StyleGAN - Official TensorFlow Implementation: https://github.com/NVlabs/stylegan\n",
    "* Stanford Vision Lab: http://vision.stanford.edu/\n",
    "* Review: Inception-v3 — 1st Runner Up (Image Classification) in ILSVRC 2015 (Tsang, 2018): https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c\n",
    "* HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models (Zhou et al., 2019): https://arxiv.org/abs/1904.01121\n",
    "* Improved Precision and Recall Metric for Assessing Generative Models (Kynkäänniemi, Karras, Laine, Lehtinen, and Aila, 2019): https://arxiv.org/abs/1904.06991\n",
    "* Large Scale GAN Training for High Fidelity Natural Image Synthesis (Brock, Donahue, and Simonyan, 2019): https://arxiv.org/abs/1809.11096\n",
    "\n",
    "From the notebook:\n",
    "\n",
    "* CelebFaces Attributes Dataset (CelebA): http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "* ImageNet: http://www.image-net.org/\n",
    "* The Fréchet Distance between Multivariate Normal Distributions (Dowson and Landau, 1982): https://core.ac.uk/reader/82269844"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2_W2\n",
    "\n",
    "Before going into the discussion on bias in machine learning, please read this case study to gain an understanding of the impact these biases can have on real lives:\n",
    "\n",
    "* Machine Bias (Angwin, Larson, Mattu, and Kirchner, 2016): https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
    "\n",
    "To understand some of the existing definitions of fairness and their relationships, please read the following paper and view the Google glossary entry for fairness:\n",
    "\n",
    "* Fairness Definitions Explained (Verma and Rubin, 2018): https://fairware.cs.umass.edu/papers/Verma.pdf\n",
    "* Machine Learning Glossary: Fairness (2020): https://developers.google.com/machine-learning/glossary/fairness\n",
    "\n",
    "\n",
    "To understand the complex nature of bias and fairness, please read the following paper describing ways they exist in artificial intelligence and machine learning:\n",
    "\n",
    "* A Survey on Bias and Fairness in Machine Learning (Mehrabi, Morstatter, Saxena, Lerman, and Galstyan, 2019): https://arxiv.org/abs/1908.09635\n",
    "\n",
    "To understand the complex nature of bias and fairness, please read the following paper describing ways they exist in artificial intelligence and machine learning:\n",
    "\n",
    "* A Survey on Bias and Fairness in Machine Learning (Mehrabi, Morstatter, Saxena, Lerman, and Galstyan, 2019): https://arxiv.org/abs/1908.09635\n",
    "\n",
    "\n",
    "All of the resources cited in Course 2 Week 2, in one place. You are encouraged to explore these papers/sites if they interest you, especially because this is an important topic to understand. They are listed in the order they appear in the lessons.\n",
    "\n",
    "From the videos:\n",
    "\n",
    "* Hyperspherical Variational Auto-Encoders (Davidson, Falorsi, De Cao, Kipf, and Tomczak, 2018): https://arxiv.org/abs/1804.00891\n",
    "* Generating Diverse High-Fidelity Images with VQ-VAE-2 (Razavi, van den Oord, and Vinyals, 2019): https://arxiv.org/abs/1906.00446\n",
    "* Conditional Image Generation with PixelCNN Decoders (van den Oord et al., 2016): https://arxiv.org/abs/1606.05328\n",
    "* Glow: Better Reversible Generative Models (Dhariwal and Kingma, 2018): https://openai.com/blog/glow/\n",
    "* Machine Bias (Angwin, Larson, Mattu, and Kirchner, 2016): https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
    "* Fairness Definitions Explained (Verma and Rubin, 2018): https://fairware.cs.umass.edu/papers/Verma.pdf\n",
    "* Does Object Recognition Work for Everyone? (DeVries, Misra, Wang, and van der Maaten, 2019): https://arxiv.org/abs/1906.02659\n",
    "* PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models (Menon, Damian, Hu, Ravi, and Rudin, 2020): https://arxiv.org/abs/2003.03808\n",
    "* What a machine learning tool that turns Obama white can (and can't) tell us about AI bias (Vincent, 2020): https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias\n",
    "\n",
    "From the notebook:\n",
    "\n",
    "* Mitigating Unwanted Biases with Adversarial Learning (Zhang, Lemoine, and Mitchell, 2018): https://m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf\n",
    "* Tutorial on Fairness Accountability Transparency and Ethics in Computer Vision at CVPR 2020 (Gebru and Denton, 2020): https://sites.google.com/view/fatecv-tutorial/schedule?authuser=0\n",
    "* Machine Learning Glossary: Fairness (2020): https://developers.google.com/machine-learning/glossary/fairness\n",
    "* CelebFaces Attributes Dataset (CelebA): http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "\n",
    "\n",
    "If you're interested in learning more about VAE's here are some useful resources:\n",
    "\n",
    "*   [$\\beta$-VAEs](https://openreview.net/forum?id=Sy2fzU9gl) showed that you can weight the KL-divergence term differently to reward \"exploration\" by the model. \n",
    "*   [VQ-VAE-2](https://arxiv.org/pdf/1906.00446.pdf) is a VAE-Autoregressive hybrid generative model, and has been ablbe to generate incredibly diverse images - keeping up with GANs. :) \n",
    "*   [VAE-GAN](https://arxiv.org/abs/1512.09300) is a VAE-GAN hybrid generative model that uses an adversarial loss (that is, the discriminator's judgments on real/fake) on a VAE. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2_W3\n",
    "\n",
    "Amazed by StyleGAN's capabilities? Take a look at the original paper! Note that it may take a few extra moments to load because of the high-resolution images.\n",
    "\n",
    "* A Style-Based Generator Architecture for Generative Adversarial Networks (Karras, Laine, and Aila, 2019): https://arxiv.org/abs/1812.04948\n",
    "\n",
    "Want another explanation of StyleGAN? This article provides a great walkthrough of StyleGAN and even discusses StyleGAN's successor: StyleGAN2!\n",
    "\n",
    "* GAN — StyleGAN & StyleGAN2 (Hui, 2020): https://medium.com/@jonathan_hui/gan-stylegan-stylegan2-479bdf256299\n",
    "\n",
    "\n",
    "All of the resources cited in Course 2 Week 3, in one place. You are encouraged to explore these papers/sites if they interest you! They are listed in the order they appear in the lessons.\n",
    "\n",
    "From the videos:\n",
    "\n",
    "* Generative Adversarial Networks (Goodfellow et al., 2014): https://arxiv.org/abs/1406.2661\n",
    "* Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford, Metz, and Chintala, 2016): https://arxiv.org/abs/1511.06434\n",
    "* Coupled Generative Adversarial Networks (Liu and Tuzel, 2016): https://arxiv.org/abs/1606.07536\n",
    "* Progressive Growing of GANs for Improved Quality, Stability, and Variation (Karras, Aila, Laine, and Lehtinen, 2018): https://arxiv.org/abs/1710.10196\n",
    "* A Style-Based Generator Architecture for Generative Adversarial Networks (Karras, Laine, and Aila, 2019): https://arxiv.org/abs/1812.04948\n",
    "* The Unusual Effectiveness of Averaging in GAN Training (Yazici et al., 2019): https://arxiv.org/abs/1806.04498v2\n",
    "* Progressive Growing of GANs for Improved Quality, Stability, and Variation (Karras, Aila, Laine, and Lehtinen, 2018): https://arxiv.org/abs/1710.10196\n",
    "* StyleGAN - Official TensorFlow Implementation (Karras et al., 2019): https://github.com/NVlabs/stylegan\n",
    "* StyleGAN Faces Training (Branwen, 2019): https://www.gwern.net/images/gan/2019-03-16-stylegan-facestraining.mp4\n",
    "* Facebook AI Proposes Group Normalization Alternative to Batch Normalization (Peng, 2018): https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7\n",
    "\n",
    "In this notebook, you're going to learn about StyleGAN2, from the paper Analyzing and Improving the Image Quality of StyleGAN (Karras et al., 2019), and how it builds on StyleGAN. This is the V2 of StyleGAN, so be prepared for even more extraordinary outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
